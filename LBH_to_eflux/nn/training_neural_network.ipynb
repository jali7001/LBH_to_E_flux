{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read me before using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was my attempt at applying neural network modeling to predicting electron total energy flux from LBHL and LBHS emission data using only 1 week of data. Because of the small amount of data, I omitted k cross fold validation and a bunch of necessary steps that would make this a big data problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler\n",
    "import glob\n",
    "import pdb\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Conjunction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_dir = os.path.join('/home/matsuo/amgeo_dev/Pseudo_ML','conjunction_data2')\n",
    "file_dir = os.path.join(hd_dir,'*.hdf5')\n",
    "conjunc_files = glob.glob(file_dir)\n",
    "conjunc_files = np.sort(conjunc_files) #sort the files by time \n",
    "\n",
    "ele_diff_energy_flux_arr,ion_diff_energy_flux_arr = np.empty((0,19)) ,np.empty((0,19))\n",
    "ele_flux, ion_flux = [],[]\n",
    "ssusi_lbhl, ssusi_lbhs = [],[]\n",
    "jds, lons, lats = [], [], []\n",
    "sat_nums, passes = [], []\n",
    "\n",
    "ssj_lbhl, ssj_lbhs = [],[]\n",
    "\n",
    "for file_name in conjunc_files:\n",
    "    with h5py.File(file_name, 'r') as f:\n",
    "\n",
    "        jds.extend(f['jds'][:])\n",
    "        passes.extend(f['pass_num'][:])\n",
    "        sat_nums.extend(f['sat_no'][:])\n",
    "        lons.extend(f['lons'][:])\n",
    "        lats.extend(f['lats'][:])\n",
    "        \n",
    "        #input data\n",
    "        ssusi_lbhl.extend(f['LBHL'][:])\n",
    "        ssusi_lbhs.extend(f['LBHS'][:])\n",
    "        #output \n",
    "        ele_diff_energy_flux_arr = np.vstack((ele_diff_energy_flux_arr, f['ele_diff_energy_flux'][:]))\n",
    "        ion_diff_energy_flux_arr = np.vstack((ion_diff_energy_flux_arr, f['ion_diff_energy_flux'][:]))\n",
    "        ele_flux.extend(f['ele_total_energy_flux'][:])\n",
    "        ion_flux.extend(f['ion_total_energy_flux'][:])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy arrays \n",
    "ele_flux, ion_flux = np.asarray(ele_flux), np.asarray(ion_flux)\n",
    "ssusi_lbhl, ssusi_lbhs = np.asarray(ssusi_lbhl), np.asarray(ssusi_lbhs)\n",
    "jds = np.asarray(jds)\n",
    "sat_nums, passes = np.asarray(sat_nums), np.asarray(passes)\n",
    "lons, lats = np.asarray(lons), np.asarray(lats)\n",
    "ssj_lbhl, ssj_lbhs = np.asarray(ssj_lbhl), np.asarray(ssj_lbhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the data and get rid of invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssusi_lbhl_mask = np.isfinite(ssusi_lbhl)\n",
    "ssusi_lbhs_mask = np.isfinite(ssusi_lbhs)\n",
    "ele_flux_mask = np.isfinite(ele_flux)\n",
    "ion_flux_mask = np.isfinite(ion_flux)\n",
    "\n",
    "mask_big = np.logical_and.reduce([ssusi_lbhl_mask, ssusi_lbhs_mask, ele_flux_mask,ion_flux_mask]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the masks\n",
    "ele_diff_energy_flux_arr = ele_diff_energy_flux_arr[mask_big]\n",
    "lons, lats = lons[mask_big], lats[mask_big]\n",
    "ssj_lbhl, ssj_lbhs = ssj_lbhl[mask_big], ssj_lbhs[mask_big]\n",
    "ele_flux, ion_flux, ssusi_lbhl, ssusi_lbhs, jds, sat_nums, passes = ele_flux[mask_big], ion_flux[mask_big], ssusi_lbhl[mask_big], ssusi_lbhs[mask_big], jds[mask_big], sat_nums[mask_big], passes[mask_big]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth ion_flux and ele_flux isolate signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate across the passs\n",
    "unique_passes = np.unique(passes)\n",
    "\n",
    "pixel_smooth = 5 \n",
    "ele_flux_smoothed, ion_flux_smoothed = np.zeros_like(ele_flux), np.zeros_like(ion_flux)\n",
    "for unique_pass in unique_passes:\n",
    "    #get the observations in the pass\n",
    "    mask = unique_pass == passes\n",
    "    ele_flux_in_pass, ion_flux_in_pass = ele_flux[mask], ion_flux[mask]\n",
    "    #smooth the eflux observations in the pass\n",
    "    ele_flux_smoothed[mask], ion_flux_smoothed[mask] = uniform_filter1d(ele_flux_in_pass,pixel_smooth),\\\n",
    "                                                        uniform_filter1d(ion_flux_in_pass,pixel_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Binary Input for Ion Flux (on when more than 0.1 ergs/cm^2/s/sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_more_ele = np.zeros_like(ion_flux_smoothed)\n",
    "ion_more_ele[ion_flux_smoothed > 0.1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate ML features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack(( ssusi_lbhl.reshape(-1,1) ,  ssusi_lbhs.reshape(-1,1), ion_more_ele.reshape(-1,1)  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ele_flux.reshape(-1,1) / (1.6e-12 * np.pi) # convert from erg to eV/ (cm2 * ster * S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use robustscaler since the data isn't quite Gaussian enough to use standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = RobustScaler()\n",
    "scaler_x = RobustScaler()\n",
    "\n",
    "scaler_x.fit(X)\n",
    "scaler_y.fit(y)\n",
    "\n",
    "X = scaler_x.transform(X)\n",
    "y = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scalers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the scalers for later analysis\n",
    "pickle.dump(scaler_x, open('X_scaler.pkl','wb'))\n",
    "pickle.dump(scaler_y, open('Y_scaler.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all of the Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('model2_inputs.h5','w')\n",
    "h5f.create_dataset('X',data = X)\n",
    "h5f.create_dataset('y',data = y)\n",
    "h5f.create_dataset('ele_flux',data = ele_flux)\n",
    "h5f.create_dataset('ion_flux_smoothed',data = ion_flux_smoothed)\n",
    "h5f.create_dataset('ele_flux_smoothed',data = ele_flux_smoothed)\n",
    "h5f.create_dataset('ion_flux',data = ion_flux)\n",
    "h5f.create_dataset('ssusi_lbhl', data = ssusi_lbhl)\n",
    "h5f.create_dataset('ssusi_lbhs', data = ssusi_lbhs)\n",
    "h5f.create_dataset('sat_nums', data = sat_nums)\n",
    "h5f.create_dataset('passes', data = passes)\n",
    "h5f.create_dataset('jds', data = jds)\n",
    "h5f.create_dataset('lons', data = lons)\n",
    "h5f.create_dataset('lats', data = lats)\n",
    "h5f.create_dataset('ele_diff_energy_flux_arr', data = ele_diff_energy_flux_arr)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really the only thing of note with this model is the usage of leakyReLu. I choice that activation function because I really wanted to enforce nonzero neuron activation and as a result enforce non linearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_inputs,n_outputs):\n",
    "    \"\"\"\n",
    "    Super Simple sequential neural network model\n",
    "    \"\"\"\n",
    "    NN_model = keras.Sequential()\n",
    "\n",
    "    # The Input Layer :\n",
    "    NN_model.add(keras.layers.Dense(8, kernel_initializer='normal',input_dim = n_inputs, activation='relu'))\n",
    "    \n",
    "    # The Hidden Layers :\n",
    "    hidden_layer = tf.keras.layers.LeakyReLU(32 ) \n",
    "    NN_model.add(hidden_layer)\n",
    "\n",
    "    # The Output Layer :\n",
    "    NN_model.add(keras.layers.Dense(n_outputs, kernel_initializer='normal',activation='linear') )\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate = 0.01)\n",
    "    \n",
    "    # Compile the network :\n",
    "    NN_model.compile(loss='mse', optimizer= opt, metrics=['mse','mean_absolute_percentage_error'])\n",
    "\n",
    "    return NN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I wanted to kep track of which passes were test and which weren't. If that's not important ignore the hardcoding here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_passes = np.unique(passes)\n",
    "len(unique_passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let 10% of passes be test passes\n",
    "test_ratio = 0.1 \n",
    "num_test_passes = int(test_ratio * len(unique_passes) ) - 1\n",
    "\n",
    "#pick random passes to be test passes\n",
    "index = np.random.choice(unique_passes, num_test_passes, replace=False) \n",
    "test_pass_nums = [ 12.,  18.,  22.,  35.,  51.,  70.,  72.,  83.,  88.,  91., 112.,\n",
    "       113., 135., 137., 150., 158., 161., 168., 184., 189., 205., 208.,\n",
    "       216., 217., 222., 228., 240., 244., 261., 263., 267., 272., 274.,\n",
    "       275., 277., 285., 306., 316., 318., 323., 337., 348., 359., 372.,\n",
    "       387., 395., 416., 420., 428., 436., 466., 481., 513., 520., 538.,\n",
    "       555., 566.]\n",
    "test_mask = np.zeros_like(passes, dtype = bool)\n",
    "for pass_ind in test_pass_nums:\n",
    "    test_mask = np.logical_or(test_mask, passes == pass_ind)\n",
    "train_mask = np.logical_not(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, y_train = y[test_mask], y[train_mask]\n",
    "X_test, X_train = X[test_mask], X[train_mask]\n",
    "jd_test = jds[test_mask] \n",
    "ssj_lbhl_test, ssj_lbhs_test = ssj_lbhl[test_mask] , ssj_lbhs[test_mask] \n",
    "sat_nums_test = sat_nums[test_mask]\n",
    "#add lat lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17304/17304 [==============================] - 10s 556us/step - loss: 46.0778 - mse: 46.0778 - mean_absolute_percentage_error: 5147.5972 - val_loss: 50.9570 - val_mse: 50.9570 - val_mean_absolute_percentage_error: 4359.6064\n",
      "Epoch 2/200\n",
      "17304/17304 [==============================] - 10s 567us/step - loss: 45.1658 - mse: 45.1658 - mean_absolute_percentage_error: 5120.1177 - val_loss: 52.5739 - val_mse: 52.5739 - val_mean_absolute_percentage_error: 5355.2075\n",
      "Epoch 3/200\n",
      "17304/17304 [==============================] - 9s 529us/step - loss: 44.9425 - mse: 44.9425 - mean_absolute_percentage_error: 4988.9722 - val_loss: 50.5599 - val_mse: 50.5599 - val_mean_absolute_percentage_error: 4835.8374\n",
      "Epoch 4/200\n",
      "17304/17304 [==============================] - 9s 530us/step - loss: 44.8637 - mse: 44.8637 - mean_absolute_percentage_error: 4772.6914 - val_loss: 56.7475 - val_mse: 56.7475 - val_mean_absolute_percentage_error: 3296.0205\n",
      "Epoch 5/200\n",
      "17304/17304 [==============================] - 9s 547us/step - loss: 45.0050 - mse: 45.0050 - mean_absolute_percentage_error: 4914.2773 - val_loss: 55.5159 - val_mse: 55.5159 - val_mean_absolute_percentage_error: 6743.2437\n",
      "Epoch 6/200\n",
      "17304/17304 [==============================] - 10s 555us/step - loss: 44.6164 - mse: 44.6164 - mean_absolute_percentage_error: 4164.0752 - val_loss: 55.5550 - val_mse: 55.5550 - val_mean_absolute_percentage_error: 5217.2319\n",
      "Epoch 7/200\n",
      "17304/17304 [==============================] - 10s 566us/step - loss: 44.7047 - mse: 44.7047 - mean_absolute_percentage_error: 4780.4111 - val_loss: 48.8749 - val_mse: 48.8749 - val_mean_absolute_percentage_error: 4267.6675\n",
      "Epoch 8/200\n",
      "17304/17304 [==============================] - 10s 562us/step - loss: 44.8926 - mse: 44.8926 - mean_absolute_percentage_error: 4620.8062 - val_loss: 51.5643 - val_mse: 51.5643 - val_mean_absolute_percentage_error: 6425.7876\n",
      "Epoch 9/200\n",
      "17304/17304 [==============================] - 11s 618us/step - loss: 44.9494 - mse: 44.9494 - mean_absolute_percentage_error: 4459.0659 - val_loss: 50.3889 - val_mse: 50.3889 - val_mean_absolute_percentage_error: 5598.8213\n",
      "Epoch 10/200\n",
      "17304/17304 [==============================] - 11s 621us/step - loss: 44.7338 - mse: 44.7338 - mean_absolute_percentage_error: 3583.9983 - val_loss: 50.5015 - val_mse: 50.5015 - val_mean_absolute_percentage_error: 5132.8306\n",
      "Epoch 11/200\n",
      "17304/17304 [==============================] - 11s 613us/step - loss: 45.0006 - mse: 45.0006 - mean_absolute_percentage_error: 5106.3770 - val_loss: 54.0264 - val_mse: 54.0264 - val_mean_absolute_percentage_error: 3746.8491\n",
      "Epoch 12/200\n",
      "17304/17304 [==============================] - 11s 611us/step - loss: 44.7509 - mse: 44.7509 - mean_absolute_percentage_error: 4655.8115 - val_loss: 51.5141 - val_mse: 51.5141 - val_mean_absolute_percentage_error: 4711.2612\n",
      "Epoch 13/200\n",
      "17304/17304 [==============================] - 11s 628us/step - loss: 44.6479 - mse: 44.6479 - mean_absolute_percentage_error: 5125.3999 - val_loss: 50.6167 - val_mse: 50.6167 - val_mean_absolute_percentage_error: 3856.5886\n",
      "Epoch 14/200\n",
      "17304/17304 [==============================] - 11s 615us/step - loss: 44.8242 - mse: 44.8242 - mean_absolute_percentage_error: 4301.0474 - val_loss: 49.8114 - val_mse: 49.8114 - val_mean_absolute_percentage_error: 6003.2456\n",
      "Epoch 15/200\n",
      "17304/17304 [==============================] - 10s 605us/step - loss: 44.6261 - mse: 44.6261 - mean_absolute_percentage_error: 4830.0718 - val_loss: 49.5409 - val_mse: 49.5409 - val_mean_absolute_percentage_error: 4262.5210\n",
      "Epoch 16/200\n",
      "17304/17304 [==============================] - 10s 552us/step - loss: 44.8110 - mse: 44.8110 - mean_absolute_percentage_error: 4639.6069 - val_loss: 49.6348 - val_mse: 49.6348 - val_mean_absolute_percentage_error: 4029.3069\n",
      "Epoch 17/200\n",
      "17304/17304 [==============================] - 10s 563us/step - loss: 44.7142 - mse: 44.7142 - mean_absolute_percentage_error: 4640.0181 - val_loss: 51.4799 - val_mse: 51.4799 - val_mean_absolute_percentage_error: 4403.6978\n",
      "Epoch 18/200\n",
      "17304/17304 [==============================] - 10s 565us/step - loss: 44.8611 - mse: 44.8611 - mean_absolute_percentage_error: 4568.6377 - val_loss: 49.6825 - val_mse: 49.6825 - val_mean_absolute_percentage_error: 6774.6348\n",
      "Epoch 19/200\n",
      "17304/17304 [==============================] - 11s 609us/step - loss: 44.5928 - mse: 44.5928 - mean_absolute_percentage_error: 4902.2446 - val_loss: 49.4869 - val_mse: 49.4869 - val_mean_absolute_percentage_error: 3979.4197\n",
      "Epoch 20/200\n",
      "17304/17304 [==============================] - 11s 627us/step - loss: 45.0097 - mse: 45.0097 - mean_absolute_percentage_error: 4383.1743 - val_loss: 53.3403 - val_mse: 53.3403 - val_mean_absolute_percentage_error: 4163.8477\n",
      "Epoch 21/200\n",
      "17304/17304 [==============================] - 10s 606us/step - loss: 44.7750 - mse: 44.7750 - mean_absolute_percentage_error: 5185.5269 - val_loss: 54.4856 - val_mse: 54.4856 - val_mean_absolute_percentage_error: 2112.0500\n",
      "Epoch 22/200\n",
      "17304/17304 [==============================] - 11s 617us/step - loss: 45.1910 - mse: 45.1910 - mean_absolute_percentage_error: 4346.8911 - val_loss: 51.1780 - val_mse: 51.1780 - val_mean_absolute_percentage_error: 4271.9253\n",
      "Epoch 23/200\n",
      "17304/17304 [==============================] - 10s 601us/step - loss: 44.7879 - mse: 44.7879 - mean_absolute_percentage_error: 4380.7788 - val_loss: 55.0285 - val_mse: 55.0285 - val_mean_absolute_percentage_error: 7037.0347\n",
      "Epoch 24/200\n",
      "17304/17304 [==============================] - 10s 566us/step - loss: 44.8269 - mse: 44.8269 - mean_absolute_percentage_error: 4264.5000 - val_loss: 49.8026 - val_mse: 49.8026 - val_mean_absolute_percentage_error: 4707.2310\n",
      "Epoch 25/200\n",
      "17304/17304 [==============================] - 10s 568us/step - loss: 44.7610 - mse: 44.7610 - mean_absolute_percentage_error: 5040.6270 - val_loss: 50.6396 - val_mse: 50.6396 - val_mean_absolute_percentage_error: 4188.8193\n",
      "Epoch 26/200\n",
      "17304/17304 [==============================] - 10s 567us/step - loss: 44.5691 - mse: 44.5691 - mean_absolute_percentage_error: 5070.3652 - val_loss: 53.1079 - val_mse: 53.1079 - val_mean_absolute_percentage_error: 4141.7803\n",
      "Epoch 27/200\n",
      "17304/17304 [==============================] - 10s 571us/step - loss: 44.8637 - mse: 44.8637 - mean_absolute_percentage_error: 4219.0479 - val_loss: 50.9880 - val_mse: 50.9880 - val_mean_absolute_percentage_error: 4021.7817\n",
      "Epoch 28/200\n",
      "17304/17304 [==============================] - 10s 565us/step - loss: 44.6319 - mse: 44.6319 - mean_absolute_percentage_error: 5001.3584 - val_loss: 50.5617 - val_mse: 50.5617 - val_mean_absolute_percentage_error: 3911.6699\n",
      "Epoch 29/200\n",
      "17304/17304 [==============================] - 10s 565us/step - loss: 44.6201 - mse: 44.6201 - mean_absolute_percentage_error: 4524.2256 - val_loss: 50.8920 - val_mse: 50.8920 - val_mean_absolute_percentage_error: 4004.7739\n",
      "Epoch 30/200\n",
      "17304/17304 [==============================] - 10s 567us/step - loss: 44.8851 - mse: 44.8851 - mean_absolute_percentage_error: 4531.2476 - val_loss: 51.3743 - val_mse: 51.3743 - val_mean_absolute_percentage_error: 4247.5537\n",
      "Epoch 31/200\n",
      "17304/17304 [==============================] - 10s 571us/step - loss: 44.5710 - mse: 44.5710 - mean_absolute_percentage_error: 4041.4438 - val_loss: 49.8941 - val_mse: 49.8941 - val_mean_absolute_percentage_error: 4895.5693\n",
      "Epoch 32/200\n",
      "17304/17304 [==============================] - 10s 563us/step - loss: 45.0051 - mse: 45.0051 - mean_absolute_percentage_error: 4647.9897 - val_loss: 49.4647 - val_mse: 49.4647 - val_mean_absolute_percentage_error: 3049.6089\n",
      "Epoch 33/200\n",
      "17304/17304 [==============================] - 10s 572us/step - loss: 44.7995 - mse: 44.7995 - mean_absolute_percentage_error: 4548.1938 - val_loss: 55.2549 - val_mse: 55.2549 - val_mean_absolute_percentage_error: 4634.9263\n",
      "Epoch 34/200\n",
      "17304/17304 [==============================] - 10s 570us/step - loss: 44.7512 - mse: 44.7512 - mean_absolute_percentage_error: 4637.3052 - val_loss: 54.0363 - val_mse: 54.0363 - val_mean_absolute_percentage_error: 6353.3052\n",
      "Epoch 35/200\n",
      "17304/17304 [==============================] - 10s 567us/step - loss: 44.9381 - mse: 44.9381 - mean_absolute_percentage_error: 5024.3325 - val_loss: 53.2809 - val_mse: 53.2809 - val_mean_absolute_percentage_error: 4547.3237\n",
      "Epoch 36/200\n",
      "17304/17304 [==============================] - 10s 593us/step - loss: 44.7349 - mse: 44.7349 - mean_absolute_percentage_error: 5467.5918 - val_loss: 54.9092 - val_mse: 54.9092 - val_mean_absolute_percentage_error: 4291.1987\n",
      "Epoch 37/200\n",
      "17304/17304 [==============================] - 11s 634us/step - loss: 44.9836 - mse: 44.9836 - mean_absolute_percentage_error: 4102.3447 - val_loss: 50.4977 - val_mse: 50.4977 - val_mean_absolute_percentage_error: 4929.1313\n",
      "Epoch 38/200\n",
      "17304/17304 [==============================] - 11s 617us/step - loss: 44.6067 - mse: 44.6067 - mean_absolute_percentage_error: 5362.3857 - val_loss: 50.3526 - val_mse: 50.3526 - val_mean_absolute_percentage_error: 4034.2122\n",
      "Epoch 39/200\n",
      "17304/17304 [==============================] - 10s 578us/step - loss: 45.1431 - mse: 45.1431 - mean_absolute_percentage_error: 4547.1377 - val_loss: 50.9381 - val_mse: 50.9381 - val_mean_absolute_percentage_error: 4248.7734\n",
      "Epoch 40/200\n",
      "17304/17304 [==============================] - 10s 579us/step - loss: 44.6771 - mse: 44.6771 - mean_absolute_percentage_error: 4891.2407 - val_loss: 51.4210 - val_mse: 51.4210 - val_mean_absolute_percentage_error: 3581.7620\n",
      "Epoch 41/200\n",
      "17304/17304 [==============================] - 9s 536us/step - loss: 44.7784 - mse: 44.7784 - mean_absolute_percentage_error: 4458.9365 - val_loss: 59.1791 - val_mse: 59.1791 - val_mean_absolute_percentage_error: 5624.9766\n",
      "Epoch 42/200\n",
      "17304/17304 [==============================] - 9s 535us/step - loss: 44.8932 - mse: 44.8932 - mean_absolute_percentage_error: 5091.2261 - val_loss: 53.7512 - val_mse: 53.7512 - val_mean_absolute_percentage_error: 4825.1284\n",
      "Epoch 43/200\n",
      "17304/17304 [==============================] - 9s 546us/step - loss: 44.9988 - mse: 44.9988 - mean_absolute_percentage_error: 4575.3413 - val_loss: 52.3771 - val_mse: 52.3771 - val_mean_absolute_percentage_error: 3489.6060\n",
      "Epoch 44/200\n",
      "17304/17304 [==============================] - 9s 538us/step - loss: 44.8302 - mse: 44.8302 - mean_absolute_percentage_error: 4530.1807 - val_loss: 51.7380 - val_mse: 51.7380 - val_mean_absolute_percentage_error: 4145.2134\n",
      "Epoch 45/200\n",
      "17304/17304 [==============================] - 9s 538us/step - loss: 44.7517 - mse: 44.7517 - mean_absolute_percentage_error: 4002.0278 - val_loss: 50.7250 - val_mse: 50.7250 - val_mean_absolute_percentage_error: 3925.1548\n",
      "Epoch 46/200\n",
      "17304/17304 [==============================] - 9s 539us/step - loss: 44.7245 - mse: 44.7245 - mean_absolute_percentage_error: 4744.6343 - val_loss: 48.6302 - val_mse: 48.6302 - val_mean_absolute_percentage_error: 3958.2441\n",
      "Epoch 47/200\n",
      "17304/17304 [==============================] - 9s 541us/step - loss: 44.4854 - mse: 44.4854 - mean_absolute_percentage_error: 4553.0908 - val_loss: 50.2257 - val_mse: 50.2257 - val_mean_absolute_percentage_error: 3557.1650\n",
      "Epoch 48/200\n",
      "17304/17304 [==============================] - 9s 540us/step - loss: 44.5936 - mse: 44.5936 - mean_absolute_percentage_error: 4657.8716 - val_loss: 50.5305 - val_mse: 50.5305 - val_mean_absolute_percentage_error: 6076.7651\n",
      "Epoch 49/200\n",
      "17304/17304 [==============================] - 9s 542us/step - loss: 44.8088 - mse: 44.8088 - mean_absolute_percentage_error: 4273.1509 - val_loss: 49.2828 - val_mse: 49.2828 - val_mean_absolute_percentage_error: 4311.5522\n",
      "Epoch 50/200\n",
      "17304/17304 [==============================] - 10s 581us/step - loss: 44.8237 - mse: 44.8237 - mean_absolute_percentage_error: 4502.5293 - val_loss: 53.3274 - val_mse: 53.3274 - val_mean_absolute_percentage_error: 5390.1924\n",
      "Epoch 51/200\n",
      "17304/17304 [==============================] - 11s 613us/step - loss: 45.0126 - mse: 45.0126 - mean_absolute_percentage_error: 4112.6108 - val_loss: 51.3025 - val_mse: 51.3025 - val_mean_absolute_percentage_error: 3327.7520\n",
      "Epoch 52/200\n",
      "17304/17304 [==============================] - 11s 615us/step - loss: 45.0268 - mse: 45.0268 - mean_absolute_percentage_error: 4652.2480 - val_loss: 48.1630 - val_mse: 48.1630 - val_mean_absolute_percentage_error: 3726.3057\n",
      "Epoch 53/200\n",
      "17304/17304 [==============================] - 10s 596us/step - loss: 44.7886 - mse: 44.7886 - mean_absolute_percentage_error: 4488.6963 - val_loss: 52.1735 - val_mse: 52.1735 - val_mean_absolute_percentage_error: 4338.1758\n",
      "Epoch 54/200\n",
      "17304/17304 [==============================] - 10s 602us/step - loss: 44.7716 - mse: 44.7716 - mean_absolute_percentage_error: 4815.0879 - val_loss: 55.2354 - val_mse: 55.2354 - val_mean_absolute_percentage_error: 3822.3984\n",
      "Epoch 55/200\n",
      "17304/17304 [==============================] - 11s 611us/step - loss: 44.2963 - mse: 44.2963 - mean_absolute_percentage_error: 4717.3740 - val_loss: 75.3349 - val_mse: 75.3349 - val_mean_absolute_percentage_error: 3681.0010\n",
      "Epoch 56/200\n",
      "17304/17304 [==============================] - 10s 605us/step - loss: 44.7028 - mse: 44.7028 - mean_absolute_percentage_error: 4705.5376 - val_loss: 53.4342 - val_mse: 53.4342 - val_mean_absolute_percentage_error: 3913.0859ean_absolute_percenta\n",
      "Epoch 57/200\n",
      "17304/17304 [==============================] - 10s 603us/step - loss: 44.7568 - mse: 44.7568 - mean_absolute_percentage_error: 4414.3906 - val_loss: 51.7013 - val_mse: 51.7013 - val_mean_absolute_percentage_error: 5150.1035\n",
      "Epoch 58/200\n",
      "17304/17304 [==============================] - 10s 603us/step - loss: 44.6238 - mse: 44.6238 - mean_absolute_percentage_error: 4729.6543 - val_loss: 48.0864 - val_mse: 48.0864 - val_mean_absolute_percentage_error: 2595.6614\n",
      "Epoch 59/200\n",
      "17304/17304 [==============================] - 10s 605us/step - loss: 45.0958 - mse: 45.0958 - mean_absolute_percentage_error: 4622.7656 - val_loss: 49.0957 - val_mse: 49.0957 - val_mean_absolute_percentage_error: 3678.8752\n",
      "Epoch 60/200\n",
      "17304/17304 [==============================] - 10s 566us/step - loss: 44.6527 - mse: 44.6527 - mean_absolute_percentage_error: 4642.0137 - val_loss: 48.9688 - val_mse: 48.9688 - val_mean_absolute_percentage_error: 4766.2183\n",
      "Epoch 61/200\n",
      "17304/17304 [==============================] - 10s 567us/step - loss: 44.7683 - mse: 44.7683 - mean_absolute_percentage_error: 4759.7573 - val_loss: 49.4361 - val_mse: 49.4361 - val_mean_absolute_percentage_error: 5160.3618\n",
      "Epoch 62/200\n",
      "17304/17304 [==============================] - 10s 570us/step - loss: 44.9089 - mse: 44.9089 - mean_absolute_percentage_error: 5231.1372 - val_loss: 50.7874 - val_mse: 50.7874 - val_mean_absolute_percentage_error: 5085.7183\n",
      "Epoch 63/200\n",
      "17304/17304 [==============================] - 10s 565us/step - loss: 44.8889 - mse: 44.8889 - mean_absolute_percentage_error: 4349.7109 - val_loss: 52.0373 - val_mse: 52.0373 - val_mean_absolute_percentage_error: 6089.7466\n",
      "Epoch 64/200\n",
      "17304/17304 [==============================] - 10s 577us/step - loss: 44.7886 - mse: 44.7886 - mean_absolute_percentage_error: 4845.9878 - val_loss: 48.8147 - val_mse: 48.8147 - val_mean_absolute_percentage_error: 3452.1316\n",
      "Epoch 65/200\n",
      "17304/17304 [==============================] - 10s 579us/step - loss: 44.6855 - mse: 44.6855 - mean_absolute_percentage_error: 4132.8740 - val_loss: 49.7515 - val_mse: 49.7515 - val_mean_absolute_percentage_error: 4184.0293\n",
      "Epoch 66/200\n",
      "17304/17304 [==============================] - 10s 566us/step - loss: 44.8095 - mse: 44.8095 - mean_absolute_percentage_error: 4775.4604 - val_loss: 54.9315 - val_mse: 54.9315 - val_mean_absolute_percentage_error: 4117.4297\n",
      "Epoch 67/200\n",
      "17304/17304 [==============================] - 10s 572us/step - loss: 44.7859 - mse: 44.7859 - mean_absolute_percentage_error: 4170.4824 - val_loss: 52.5116 - val_mse: 52.5116 - val_mean_absolute_percentage_error: 3233.0757\n",
      "Epoch 68/200\n",
      "17304/17304 [==============================] - 11s 639us/step - loss: 44.6943 - mse: 44.6943 - mean_absolute_percentage_error: 4115.7798 - val_loss: 49.7081 - val_mse: 49.7081 - val_mean_absolute_percentage_error: 2974.6892\n",
      "Epoch 69/200\n",
      "17304/17304 [==============================] - 11s 613us/step - loss: 45.0428 - mse: 45.0428 - mean_absolute_percentage_error: 3841.3777 - val_loss: 50.9627 - val_mse: 50.9627 - val_mean_absolute_percentage_error: 5291.0166\n",
      "Epoch 70/200\n",
      "17304/17304 [==============================] - 10s 569us/step - loss: 44.6175 - mse: 44.6175 - mean_absolute_percentage_error: 4736.9199 - val_loss: 50.0727 - val_mse: 50.0727 - val_mean_absolute_percentage_error: 2972.6123\n",
      "Epoch 71/200\n",
      "17304/17304 [==============================] - 10s 567us/step - loss: 44.6866 - mse: 44.6866 - mean_absolute_percentage_error: 4928.8677 - val_loss: 51.9668 - val_mse: 51.9668 - val_mean_absolute_percentage_error: 5647.1401\n",
      "Epoch 72/200\n",
      "17304/17304 [==============================] - 10s 574us/step - loss: 45.0825 - mse: 45.0825 - mean_absolute_percentage_error: 4162.0283 - val_loss: 50.4619 - val_mse: 50.4619 - val_mean_absolute_percentage_error: 3065.3372\n",
      "Epoch 73/200\n",
      "17304/17304 [==============================] - 10s 569us/step - loss: 44.7784 - mse: 44.7784 - mean_absolute_percentage_error: 4600.7695 - val_loss: 53.7633 - val_mse: 53.7633 - val_mean_absolute_percentage_error: 4081.4749\n",
      "Epoch 74/200\n",
      "17304/17304 [==============================] - 10s 569us/step - loss: 44.5646 - mse: 44.5646 - mean_absolute_percentage_error: 4099.4766 - val_loss: 55.0230 - val_mse: 55.0230 - val_mean_absolute_percentage_error: 3505.1726\n",
      "Epoch 75/200\n",
      "17304/17304 [==============================] - 10s 568us/step - loss: 44.7874 - mse: 44.7874 - mean_absolute_percentage_error: 4625.9473 - val_loss: 49.6748 - val_mse: 49.6748 - val_mean_absolute_percentage_error: 5311.2256\n",
      "Epoch 76/200\n",
      "17304/17304 [==============================] - 10s 569us/step - loss: 44.5607 - mse: 44.5607 - mean_absolute_percentage_error: 4753.9492 - val_loss: 50.5167 - val_mse: 50.5167 - val_mean_absolute_percentage_error: 3608.7903\n",
      "Epoch 77/200\n",
      "17304/17304 [==============================] - 10s 571us/step - loss: 44.8682 - mse: 44.8682 - mean_absolute_percentage_error: 4712.9160 - val_loss: 49.7145 - val_mse: 49.7145 - val_mean_absolute_percentage_error: 2952.7373\n",
      "Epoch 78/200\n",
      "17304/17304 [==============================] - 10s 570us/step - loss: 44.7150 - mse: 44.7150 - mean_absolute_percentage_error: 4712.9834 - val_loss: 51.0410 - val_mse: 51.0410 - val_mean_absolute_percentage_error: 2908.2996\n",
      "Epoch 79/200\n",
      "17304/17304 [==============================] - 10s 574us/step - loss: 44.9485 - mse: 44.9485 - mean_absolute_percentage_error: 5056.8804 - val_loss: 52.4868 - val_mse: 52.4868 - val_mean_absolute_percentage_error: 3536.8896\n",
      "Epoch 80/200\n",
      "17304/17304 [==============================] - 10s 569us/step - loss: 44.7724 - mse: 44.7724 - mean_absolute_percentage_error: 4147.8140 - val_loss: 49.7690 - val_mse: 49.7690 - val_mean_absolute_percentage_error: 3758.6128\n",
      "Epoch 81/200\n",
      "17304/17304 [==============================] - 10s 573us/step - loss: 44.6444 - mse: 44.6444 - mean_absolute_percentage_error: 4471.3350 - val_loss: 56.1838 - val_mse: 56.1838 - val_mean_absolute_percentage_error: 5122.0293\n",
      "Epoch 82/200\n",
      "17304/17304 [==============================] - 10s 571us/step - loss: 44.8293 - mse: 44.8293 - mean_absolute_percentage_error: 5545.9600 - val_loss: 52.3089 - val_mse: 52.3089 - val_mean_absolute_percentage_error: 4223.3354\n",
      "Epoch 83/200\n",
      "17304/17304 [==============================] - 10s 571us/step - loss: 44.7871 - mse: 44.7871 - mean_absolute_percentage_error: 4327.9824 - val_loss: 53.7721 - val_mse: 53.7721 - val_mean_absolute_percentage_error: 5972.7153\n",
      "Epoch 84/200\n",
      "17304/17304 [==============================] - 10s 570us/step - loss: 44.7469 - mse: 44.7469 - mean_absolute_percentage_error: 4826.6304 - val_loss: 49.8724 - val_mse: 49.8724 - val_mean_absolute_percentage_error: 6546.7803\n",
      "Epoch 85/200\n",
      "17304/17304 [==============================] - 10s 570us/step - loss: 44.8578 - mse: 44.8578 - mean_absolute_percentage_error: 4221.0645 - val_loss: 52.4936 - val_mse: 52.4936 - val_mean_absolute_percentage_error: 3177.5396\n",
      "Epoch 86/200\n",
      "17304/17304 [==============================] - 10s 570us/step - loss: 44.7980 - mse: 44.7980 - mean_absolute_percentage_error: 4627.8921 - val_loss: 54.9029 - val_mse: 54.9029 - val_mean_absolute_percentage_error: 5096.0400\n",
      "Epoch 87/200\n",
      "17304/17304 [==============================] - 10s 575us/step - loss: 44.7382 - mse: 44.7382 - mean_absolute_percentage_error: 5252.8628 - val_loss: 50.4297 - val_mse: 50.4297 - val_mean_absolute_percentage_error: 4180.8501\n",
      "Epoch 88/200\n",
      "17304/17304 [==============================] - 10s 573us/step - loss: 44.4803 - mse: 44.4803 - mean_absolute_percentage_error: 4282.7363 - val_loss: 49.3567 - val_mse: 49.3567 - val_mean_absolute_percentage_error: 2558.2153\n",
      "Epoch 89/200\n",
      "17304/17304 [==============================] - 10s 570us/step - loss: 44.8918 - mse: 44.8918 - mean_absolute_percentage_error: 4709.7788 - val_loss: 48.7893 - val_mse: 48.7893 - val_mean_absolute_percentage_error: 3322.3638\n",
      "Epoch 90/200\n",
      "17304/17304 [==============================] - 10s 572us/step - loss: 44.6219 - mse: 44.6219 - mean_absolute_percentage_error: 4712.0630 - val_loss: 51.6626 - val_mse: 51.6626 - val_mean_absolute_percentage_error: 5219.0537\n",
      "Epoch 91/200\n",
      "17304/17304 [==============================] - 10s 573us/step - loss: 44.5953 - mse: 44.5953 - mean_absolute_percentage_error: 4431.2539 - val_loss: 49.0827 - val_mse: 49.0827 - val_mean_absolute_percentage_error: 2401.5139\n",
      "Epoch 92/200\n",
      "17304/17304 [==============================] - 10s 571us/step - loss: 44.8165 - mse: 44.8165 - mean_absolute_percentage_error: 4473.0942 - val_loss: 52.0486 - val_mse: 52.0486 - val_mean_absolute_percentage_error: 4654.0137\n",
      "Epoch 93/200\n",
      "17304/17304 [==============================] - 10s 571us/step - loss: 44.7161 - mse: 44.7161 - mean_absolute_percentage_error: 4450.1733 - val_loss: 50.9178 - val_mse: 50.9178 - val_mean_absolute_percentage_error: 5360.8140\n",
      "Epoch 94/200\n",
      "17304/17304 [==============================] - 10s 574us/step - loss: 44.6501 - mse: 44.6501 - mean_absolute_percentage_error: 4633.2480 - val_loss: 50.2846 - val_mse: 50.2846 - val_mean_absolute_percentage_error: 3540.3716\n",
      "Epoch 95/200\n",
      "17304/17304 [==============================] - 10s 573us/step - loss: 44.8163 - mse: 44.8163 - mean_absolute_percentage_error: 5421.8032 - val_loss: 48.9224 - val_mse: 48.9224 - val_mean_absolute_percentage_error: 3839.6082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3a10a43f98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get training data\n",
    "index = np.random.choice(np.arange(len(X_train)), int(test_ratio * len(X_train) ), replace=False)  \n",
    "X_train_percent = X_train[index]\n",
    "y_train_percent = y_train[index]\n",
    "#train the nn\n",
    "model = get_model(3,1)\n",
    "\n",
    "#this callbackwill stop the training when there is no improvement in validation loss for 5 consecutive epochs\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 20,restore_best_weights = True)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 40,restore_best_weights = True)\n",
    "\n",
    "model.fit(X, y, epochs=200, batch_size=32, validation_split = 0.2, callbacks = [callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matsuo/anaconda2/envs/wirms/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/matsuo/anaconda2/envs/wirms/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: lbh_ion_mask_to_ele_flux_3/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('lbh_ion_mask_to_ele_flux_3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (wirms)",
   "language": "python",
   "name": "wirms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
