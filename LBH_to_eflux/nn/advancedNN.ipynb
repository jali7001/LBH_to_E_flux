{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "import os\n",
    "from tensorflow import keras\n",
    "import pdb\n",
    "import glob\n",
    "import pandas as pd\n",
    "from geospacepy import special_datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Conjunction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_dir = os.path.join('/home/matsuo/amgeo_dev/LBH_to_eflux/LBH_to_eflux/','conjunction_data')\n",
    "file_dir = os.path.join(hd_dir,'*.hdf5')\n",
    "conjunc_files = glob.glob(file_dir)\n",
    "conjunc_files = np.sort(conjunc_files) #sort the files by time \n",
    "\n",
    "ele_diff_energy_flux_arr,ion_diff_energy_flux_arr = np.empty((0,19)) ,np.empty((0,19))\n",
    "ele_flux, ion_flux = [],[]\n",
    "ele_mean, ion_mean = [],[]\n",
    "ssusi_lbhl, ssusi_lbhs, ssusi_lyman = [],[], []\n",
    "jds, lons, lats = [], [], []\n",
    "sat_nums, passes, hemis = [], [], []\n",
    "\n",
    "for file_name in conjunc_files:\n",
    "    with h5py.File(file_name, 'r') as f:\n",
    "        jds.extend(f['jds'][:])\n",
    "        passes.extend(f['pass_num'][:])\n",
    "        sat_nums.extend(f['sat_no'][:])\n",
    "        lons.extend(f['lons'][:])\n",
    "        lats.extend(f['lats'][:])\n",
    "#         hemis.extend(f['hemi'][:])\n",
    "        #input data\n",
    "        ssusi_lbhl.extend(f['LBHL_interped'][:])\n",
    "        ssusi_lbhs.extend(f['LBHS_interped'][:])\n",
    "        ssusi_lyman.extend(f['LYMAN_interped'][:])\n",
    "        #output \n",
    "\n",
    "        ele_flux.extend(f['ele_total_energy_flux'][:])\n",
    "        ion_flux.extend(f['ion_total_energy_flux'][:])\n",
    "        \n",
    "        ele_mean.extend(f['ele_mean_energy'][:])\n",
    "        ion_mean.extend(f['ion_mean_energy'][:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'jds': jds, 'passes': passes, 'sat_nums' : sat_nums, 'lons' : lons, 'lats' : lats, \n",
    "     'lbhl' : ssusi_lbhl,'lbhs' : ssusi_lbhs, 'lyman' : ssusi_lyman,\n",
    "    'ion_total_flux' : ion_flux, 'ele_total_flux' : ele_flux}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enddt = special_datetime.jd2datetime(np.nanmax(df['jds']))\n",
    "startdt = special_datetime.jd2datetime(np.nanmin(df['jds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_passes = np.unique(df['passes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomagnetic Indices Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get IMF components and useful activity indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this I use a useful tool by Liam Kilcommons that allows you to autodownload NASA omniweb data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/matsuo/amgeo_dev/AMGeO/src/nasaomnireader/nasaomnireader/__init__.py\", line 5, in <module>\n",
      "    from nasaomnireader.omnireader_config import config\n",
      "ModuleNotFoundError: No module named 'nasaomnireader.omnireader_config'\n",
      "\n",
      "Solar wind data files will be saved to /home/matsuo/.local/share/nasaomnireader\n"
     ]
    }
   ],
   "source": [
    "from nasaomnireader import omnireader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created interval between 2014-02-17 and 2014-02-23, cadence 5min, start index 4610, end index 6624\n"
     ]
    }
   ],
   "source": [
    "def download_omni_data(startdt,enddt):\n",
    "    omni_data = {}\n",
    "    freq = '5min'\n",
    "    indices =['BY_GSM','BZ_GSM','AE_INDEX','AL_INDEX']\n",
    "    omniInt = omnireader.omni_interval(startdt,enddt,freq)\n",
    "    jd_arr = omniInt['Epoch']\n",
    "    for index in indices:\n",
    "        omni_data[index] = omniInt[index] \n",
    "    return jd_arr,omni_data\n",
    "\n",
    "jd_arr, omni_data = download_omni_data(startdt,enddt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate observations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to interpolate these index values to the observation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying smoothing to the time series helps reduce noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of nan rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_electron_flux_model(shape(2), name = 'FUV'):\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "def get_model(n_inputs,n_outputs):\n",
    "    \n",
    "    inputs = keras.Input(shape=(3),name ='FUV')\n",
    "\n",
    "#     lbh_input = keras.Input(shape=(2), name = 'LBH')(inputs[0,1])\n",
    "#     lbh_and_lyman_input = keras.Input(shape=(1), name = 'LBH_and_Lyman')\n",
    "    je1_first_layer = keras.layers.Dense(8, kernel_initializer='normal',activation = 'relu')(inputs)\n",
    "    je1_hidden_layer = keras.layers.LeakyReLU(32)(je1_first_layer)\n",
    "    je1_output = keras.layers.Dense(1)(je1_hidden_layer)\n",
    "    \n",
    "    \n",
    "    ji1_first_layer = keras.layers.Dense(8, kernel_initializer='normal',activation = 'relu')(inputs)\n",
    "    ji1_hidden_layer = keras.layers.LeakyReLU(32)(ji1_first_layer)\n",
    "    ji1_output = keras.layers.Dense(1)(ji1_hidden_layer)\n",
    "    \n",
    "    je2_input = keras.layers.Concatenate(axis=-1)([inputs,je1_output,ji1_output])\n",
    "    je2_first_layer = keras.layers.Dense(8, kernel_initializer='normal',activation = 'relu')(je2_input)\n",
    "    je2_hidden_layer = keras.layers.LeakyReLU(32)(je2_first_layer)\n",
    "    je2_output = keras.layers.Dense(1)(je2_hidden_layer)\n",
    "\n",
    "    ji2_input = keras.layers.Concatenate(axis=-1)([inputs,je1_output,ji1_output])\n",
    "    ji2_first_layer = keras.layers.Dense(8, kernel_initializer='normal',activation = 'relu')(ji2_input)\n",
    "    ji2_hidden_layer = keras.layers.LeakyReLU(32)(ji2_first_layer)\n",
    "    ji2_output = keras.layers.Dense(1)(ji2_hidden_layer)\n",
    "    \n",
    "    outputs = keras.layers.Concatenate(axis=-1)([je2_output,ji2_output])\n",
    "    model = keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.01)\n",
    "    loss = [keras.losses.MeanSquaredError()]\n",
    "    # Compile the network :\n",
    "    model.compile(loss='mse', optimizer= optimizer, metrics=['mse','mean_absolute_percentage_error'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"my_first_model.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training with K folds Cross Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
